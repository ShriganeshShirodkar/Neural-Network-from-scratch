{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Dataset\n",
    "    - a training set of m_train images labelled as cat (1) or non-cat (0)\n",
    "    - a test set of m_test images labelled as cat and non-cat\n",
    "    - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_data():\n",
    "    train_dataset = h5py.File('datasets/train.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) \n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) \n",
    "\n",
    "    test_dataset = h5py.File('datasets/test.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) \n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) \n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T  \n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data \n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach\n",
    "1. Initialize parameters / Define hyperparameters\n",
    "2. Loop for num_iterations:\n",
    "    a. Forward propagation\n",
    "    b. Compute cost function\n",
    "    c. Backward propagation\n",
    "    d. Update parameters (using parameters, and grads from backprop) \n",
    "4. Use trained parameters to predict labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), \n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing parameters\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward and backword prop\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                 \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 \n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1] #  5-layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "\n",
    "    parameters=initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "\n",
    "        AL, caches=L_model_forward(X, parameters)\n",
    "\n",
    "        \n",
    "        # Compute cost\n",
    "        cost=compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads=L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters=update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.771749\n",
      "Cost after iteration 100: 0.672053\n",
      "Cost after iteration 200: 0.648263\n",
      "Cost after iteration 300: 0.611507\n",
      "Cost after iteration 400: 0.567047\n",
      "Cost after iteration 500: 0.540138\n",
      "Cost after iteration 600: 0.527930\n",
      "Cost after iteration 700: 0.465477\n",
      "Cost after iteration 800: 0.369126\n",
      "Cost after iteration 900: 0.391747\n",
      "Cost after iteration 1000: 0.315187\n",
      "Cost after iteration 1100: 0.272700\n",
      "Cost after iteration 1200: 0.237419\n",
      "Cost after iteration 1300: 0.199601\n",
      "Cost after iteration 1400: 0.189263\n",
      "Cost after iteration 1500: 0.161189\n",
      "Cost after iteration 1600: 0.148214\n",
      "Cost after iteration 1700: 0.137775\n",
      "Cost after iteration 1800: 0.129740\n",
      "Cost after iteration 1900: 0.121225\n",
      "Cost after iteration 2000: 0.113821\n",
      "Cost after iteration 2100: 0.107839\n",
      "Cost after iteration 2200: 0.102855\n",
      "Cost after iteration 2300: 0.100897\n",
      "Cost after iteration 2400: 0.092878\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9856459330143539\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
